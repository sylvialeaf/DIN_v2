#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
DDP (DistributedDataParallel) äº‘ç«¯ GPU å®éªŒè„šæœ¬

æ”¯æŒè‡ªåŠ¨å¯åŠ¨ DDPï¼Œæ— éœ€æ‰‹åŠ¨ä½¿ç”¨ torchrunï¼

ä½¿ç”¨æ–¹æ³•ï¼ˆç›´æ¥è¿è¡Œï¼Œä¼šè‡ªåŠ¨å¯åŠ¨ DDPï¼‰:
    python run_ddp.py                        # è¿è¡Œæ‰€æœ‰å®éªŒï¼ˆä¸¤ä¸ªæ•°æ®é›†ï¼‰
    python run_ddp.py --dataset ml-100k      # åªè¿è¡Œ ml-100k
    python run_ddp.py --dataset ml-1m        # åªè¿è¡Œ ml-1m
    python run_ddp.py --quick                # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
    python run_ddp.py --exp1                 # åªè¿è¡Œå®éªŒ1ï¼ˆå¿«æ·æ–¹å¼ï¼‰
    python run_ddp.py --exp 1                # åªè¿è¡Œå®éªŒ1ï¼ˆå®Œæ•´å‚æ•°ï¼‰
    python run_ddp.py --exp 1,2,3            # è¿è¡Œå®éªŒ1-3
    python run_ddp.py --exp2 --dataset ml-100k  # å®éªŒ2 + ml-100k
    
DDP vs DataParallel:
    - DDP å¿« 30-50%
    - GPU åˆ©ç”¨ç‡æ›´é«˜ (70-90% vs 15-30%)
    - å®Œå…¨é¿å… pack_padded_sequence é—®é¢˜

è¾“å‡ºå’ŒåŸç‰ˆ run_all_gpu.py å®Œå…¨ä¸€è‡´ï¼š
    - ç»“æœä¿å­˜åˆ° results_gpu/
    - TensorBoard æ—¥å¿—ä¿å­˜åˆ° /root/tf-logsï¼ˆAutoDLï¼‰æˆ– ./runs
"""

import os
import sys
import subprocess

# ========================================
# è‡ªåŠ¨å¯åŠ¨ DDP çš„å…¥å£ç‚¹
# ========================================

def is_launched_by_torchrun():
    """æ£€æŸ¥æ˜¯å¦å·²ç»é€šè¿‡ torchrun å¯åŠ¨"""
    return 'LOCAL_RANK' in os.environ


def auto_launch_ddp():
    """è‡ªåŠ¨ä½¿ç”¨ torchrun å¯åŠ¨ DDP"""
    import torch
    
    if not torch.cuda.is_available():
        print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å• CPU æ¨¡å¼")
        return False
    
    num_gpus = torch.cuda.device_count()
    if num_gpus <= 1:
        print(f"â„¹ï¸ æ£€æµ‹åˆ° {num_gpus} ä¸ª GPUï¼Œå°†ä½¿ç”¨å• GPU æ¨¡å¼")
        return False
    
    print(f"ğŸš€ æ£€æµ‹åˆ° {num_gpus} ä¸ª GPUï¼Œè‡ªåŠ¨å¯åŠ¨ DDP...")
    print("=" * 60)
    
    # æ„å»º torchrun å‘½ä»¤
    script_path = os.path.abspath(__file__)
    cmd = [
        sys.executable, '-m', 'torch.distributed.run',
        f'--nproc_per_node={num_gpus}',
        '--master_port=29500',
        script_path
    ] + sys.argv[1:]  # ä¼ é€’åŸå§‹å‚æ•°
    
    print(f"æ‰§è¡Œ: {' '.join(cmd)}")
    print("=" * 60)
    
    # æ‰§è¡Œå¹¶ç­‰å¾…å®Œæˆ
    result = subprocess.run(cmd)
    sys.exit(result.returncode)


# å¦‚æœä¸æ˜¯ torchrun å¯åŠ¨ï¼Œåˆ™è‡ªåŠ¨å¯åŠ¨
if not is_launched_by_torchrun():
    auto_launch_ddp()


# ========================================
# ä»¥ä¸‹æ˜¯ DDP Worker çš„ä¸»ä»£ç 
# ========================================

import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
import numpy as np
import pandas as pd
from datetime import datetime
import json
import time
from tqdm import tqdm
import platform
import multiprocessing

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from data_loader import RichFeatureDataset, get_topk_eval_data, build_topk_batch_multi
from models import DINRichLite, SimpleAveragePoolingRich, GRU4Rec, SASRec, NARM, AttentionLayer
from trainer import RichTrainer, measure_inference_speed_rich
from feature_engineering import FeatureProcessor, InteractionFeatureExtractor

try:
    import lightgbm as lgb
    HAS_LIGHTGBM = True
except ImportError:
    HAS_LIGHTGBM = False

# TensorBoard
try:
    from torch.utils.tensorboard import SummaryWriter
    HAS_TENSORBOARD = True
except ImportError:
    HAS_TENSORBOARD = False


# ========================================
# Top-K è¯„ä¼°æŒ‡æ ‡å‡½æ•°
# ========================================

def hit_at_k(ranked_items, ground_truth, k):
    """Hit Rate @ K"""
    return 1.0 if ground_truth in ranked_items[:k] else 0.0


def ndcg_at_k(ranked_items, ground_truth, k):
    """NDCG @ K"""
    for i, item in enumerate(ranked_items[:k]):
        if item == ground_truth:
            return 1.0 / np.log2(i + 2)
    return 0.0


def mrr_at_k(ranked_items, ground_truth, k):
    """MRR @ K"""
    for i, item in enumerate(ranked_items[:k]):
        if item == ground_truth:
            return 1.0 / (i + 1)
    return 0.0


def precision_at_k(ranked_items, ground_truth, k):
    """Precision @ K (å• ground truth åœºæ™¯ï¼Œå‘½ä¸­åˆ™ä¸º 1/k)"""
    hits = 1 if ground_truth in ranked_items[:k] else 0
    return hits / k


# ========================================
# DDP å·¥å…·å‡½æ•°
# ========================================

def setup_ddp():
    """åˆå§‹åŒ– DDP ç¯å¢ƒ"""
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    world_size = int(os.environ.get('WORLD_SIZE', 1))
    rank = int(os.environ.get('RANK', 0))
    
    if world_size > 1:
        dist.init_process_group(backend='nccl', init_method='env://')
        torch.cuda.set_device(local_rank)
    
    return local_rank, world_size, rank


def cleanup_ddp():
    """æ¸…ç† DDP ç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def is_main_process(rank):
    """åˆ¤æ–­æ˜¯å¦æ˜¯ä¸»è¿›ç¨‹ï¼ˆrank 0 è´Ÿè´£è¾“å‡ºå’Œä¿å­˜ï¼‰"""
    return rank == 0


def print_main(msg, rank):
    """ä»…ä¸»è¿›ç¨‹æ‰“å°"""
    if is_main_process(rank):
        print(msg)


def barrier():
    """åŒæ­¥æ‰€æœ‰è¿›ç¨‹"""
    if dist.is_initialized():
        dist.barrier()


# ========================================
# é…ç½®è§£æ
# ========================================

parser = argparse.ArgumentParser(description='DDP äº‘ç«¯ GPU å®Œæ•´å®éªŒ')
parser.add_argument('--dataset', type=str, default='both', 
                    choices=['ml-100k', 'ml-1m', 'both'])
parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼')
parser.add_argument('--epochs', type=int, default=50)
parser.add_argument('--exp', type=str, default='all', help='å®éªŒç¼–å·: 1, 2, 3, 4, 1,2,3, all')
parser.add_argument('--no-topk', action='store_true', help='ç¦ç”¨ Top-K è¯„ä¼°')
parser.add_argument('--topk-sample', type=str, default='auto')
parser.add_argument('--exp4-part', type=str, default='all', 
                    choices=['all', 'adaptive', 'contrastive'])
# å¿«æ·å‚æ•°
parser.add_argument('--exp1', action='store_true', help='å¿«æ·æ–¹å¼: ä»…è¿è¡Œå®éªŒ1')
parser.add_argument('--exp2', action='store_true', help='å¿«æ·æ–¹å¼: ä»…è¿è¡Œå®éªŒ2')
parser.add_argument('--exp3', action='store_true', help='å¿«æ·æ–¹å¼: ä»…è¿è¡Œå®éªŒ3')
parser.add_argument('--exp4', action='store_true', help='å¿«æ·æ–¹å¼: ä»…è¿è¡Œå®éªŒ4')
args = parser.parse_args()

# å¤„ç†å¿«æ·å‚æ•°
if args.exp1:
    args.exp = '1'
elif args.exp2:
    args.exp = '2'
elif args.exp3:
    args.exp = '3'
elif args.exp4:
    args.exp = '4'


# ========================================
# ä¸»å‡½æ•°
# ========================================

def main():
    # åˆå§‹åŒ– DDP
    local_rank, world_size, rank = setup_ddp()
    device = f'cuda:{local_rank}' if torch.cuda.is_available() else 'cpu'
    
    # è§£æå®éªŒ
    if args.exp == 'all':
        EXPERIMENTS_TO_RUN = [1, 2, 3, 4]
    else:
        EXPERIMENTS_TO_RUN = [int(x.strip()) for x in args.exp.split(',')]
    
    ENABLE_TOPK = not args.no_topk
    
    # é…ç½®å‚æ•°
    if args.quick:
        EPOCHS = 10
        SEQ_LENGTHS = [20, 50]
        BATCH_SIZE_PER_GPU = 1024
    else:
        EPOCHS = args.epochs
        SEQ_LENGTHS = [20, 50, 100, 150]
        BATCH_SIZE_PER_GPU = 2048  # DDP: æ¯ä¸ª GPU çš„ batch size
    
    EFFECTIVE_BATCH_SIZE = BATCH_SIZE_PER_GPU * world_size
    EMBEDDING_DIM = 64
    NUM_WORKERS = 4  # æ¯ä¸ªè¿›ç¨‹çš„ workers
    PREFETCH_FACTOR = 4
    TOPK_VALUES = [5, 10, 20]
    NUM_NEG_SAMPLES = 99
    MODELS_TO_TEST = ['DIN', 'GRU4Rec', 'SASRec', 'NARM', 'AvgPool']
    
    # TensorBoard ç›®å½•
    if platform.system() == 'Linux' and os.path.exists('/root'):
        TENSORBOARD_LOG_DIR = '/root/tf-logs'
    else:
        TENSORBOARD_LOG_DIR = './runs'
    
    # æ•°æ®é›†
    if args.dataset == 'both':
        DATASETS = ['ml-100k', 'ml-1m']
    else:
        DATASETS = [args.dataset]
    
    # ç»“æœç›®å½•
    RESULTS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'results_gpu')
    if is_main_process(rank):
        os.makedirs(RESULTS_DIR, exist_ok=True)
    
    # æ‰“å°é…ç½®ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    print_main("=" * 80, rank)
    print_main("ğŸš€ DDP åˆ†å¸ƒå¼è®­ç»ƒ", rank)
    print_main("=" * 80, rank)
    print_main(f"World Size: {world_size} GPUs", rank)
    if torch.cuda.is_available():
        for i in range(world_size):
            if rank == i:
                print(f"[Rank {rank}] GPU {local_rank}: {torch.cuda.get_device_name(local_rank)}")
            barrier()
    print_main(f"æ•°æ®é›†: {DATASETS}", rank)
    print_main(f"å®éªŒ: {EXPERIMENTS_TO_RUN}", rank)
    print_main(f"Epochs: {EPOCHS}", rank)
    print_main(f"åºåˆ—é•¿åº¦: {SEQ_LENGTHS}", rank)
    print_main(f"Batch Size: {BATCH_SIZE_PER_GPU} Ã— {world_size} = {EFFECTIVE_BATCH_SIZE}", rank)
    print_main(f"Workers per GPU: {NUM_WORKERS}", rank)
    print_main(f"æ¨¡å‹: {MODELS_TO_TEST}", rank)
    print_main(f"Top-K è¯„ä¼°: {'å¯ç”¨' if ENABLE_TOPK else 'ç¦ç”¨'}", rank)
    print_main(f"TensorBoard: {TENSORBOARD_LOG_DIR}", rank)
    print_main("=" * 80, rank)
    
    # å¼€å§‹å®éªŒ
    experiment_start = datetime.now()
    all_results = []
    
    for dataset_name in DATASETS:
        print_main(f"\n{'='*80}", rank)
        print_main(f"ğŸ“Š æ•°æ®é›†: {dataset_name}", rank)
        print_main(f"{'='*80}", rank)
        
        # å®éªŒ1: åºåˆ—é•¿åº¦ + æ¨¡å‹å¯¹æ¯”
        if 1 in EXPERIMENTS_TO_RUN:
            results1 = run_experiment1(
                dataset_name, device, local_rank, world_size, rank,
                EPOCHS, SEQ_LENGTHS, BATCH_SIZE_PER_GPU, EMBEDDING_DIM,
                NUM_WORKERS, PREFETCH_FACTOR, MODELS_TO_TEST,
                ENABLE_TOPK, TOPK_VALUES, NUM_NEG_SAMPLES,
                TENSORBOARD_LOG_DIR
            )
            all_results.extend(results1)
        
        # å®éªŒ2: æ–¹æ³•å¯¹æ¯”ï¼ˆæ··åˆç²¾æ’ï¼‰
        if 2 in EXPERIMENTS_TO_RUN:
            results2 = run_experiment2(
                dataset_name, device, local_rank, world_size, rank,
                EPOCHS, BATCH_SIZE_PER_GPU, EMBEDDING_DIM,
                NUM_WORKERS, PREFETCH_FACTOR,
                TENSORBOARD_LOG_DIR,
                ENABLE_TOPK, TOPK_VALUES
            )
            all_results.extend(results2)
        
        # å®éªŒ3: æ¶ˆèå®éªŒ
        if 3 in EXPERIMENTS_TO_RUN:
            results3 = run_experiment3(
                dataset_name, device, local_rank, world_size, rank,
                EPOCHS, BATCH_SIZE_PER_GPU, EMBEDDING_DIM,
                NUM_WORKERS, PREFETCH_FACTOR,
                TENSORBOARD_LOG_DIR,
                ENABLE_TOPK, TOPK_VALUES
            )
            all_results.extend(results3)
        
        # å®éªŒ4: é«˜çº§æ”¹è¿›
        if 4 in EXPERIMENTS_TO_RUN:
            results4 = run_experiment4(
                dataset_name, device, local_rank, world_size, rank,
                EPOCHS, BATCH_SIZE_PER_GPU, EMBEDDING_DIM,
                NUM_WORKERS, PREFETCH_FACTOR,
                TENSORBOARD_LOG_DIR, args.exp4_part
            )
            all_results.extend(results4)
    
    # ä¿å­˜ç»“æœï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    if is_main_process(rank) and all_results:
        experiment_end = datetime.now()
        total_time = (experiment_end - experiment_start).total_seconds()
        
        df_results = pd.DataFrame(all_results)
        timestamp = experiment_start.strftime('%Y%m%d_%H%M%S')
        
        # CSV
        csv_file = os.path.join(RESULTS_DIR, f'ddp_results_{timestamp}.csv')
        df_results.to_csv(csv_file, index=False)
        
        # JSON æŠ¥å‘Š
        report = {
            'timestamp': timestamp,
            'mode': 'DDP',
            'world_size': world_size,
            'device': device,
            'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',
            'datasets': DATASETS,
            'experiments': EXPERIMENTS_TO_RUN,
            'epochs': EPOCHS,
            'seq_lengths': SEQ_LENGTHS,
            'batch_size_per_gpu': BATCH_SIZE_PER_GPU,
            'effective_batch_size': EFFECTIVE_BATCH_SIZE,
            'models': MODELS_TO_TEST,
            'topk_values': TOPK_VALUES,
            'total_time_minutes': total_time / 60,
            'num_results': len(all_results),
            'results': all_results
        }
        
        json_file = os.path.join(RESULTS_DIR, f'ddp_report_{timestamp}.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "=" * 80)
        print("ğŸ“‹ å®éªŒå®Œæˆï¼")
        print("=" * 80)
        print(f"æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
        print(f"å®éªŒæ•°é‡: {len(all_results)}")
        print(f"\nğŸ“‚ ç»“æœæ–‡ä»¶:")
        print(f"   {csv_file}")
        print(f"   {json_file}")
        
        # æœ€ä½³ç»“æœ
        df_success = df_results[df_results['status'] == 'success']
        print("\nğŸ“Š å„å®éªŒæœ€ä½³ç»“æœ:")
        for exp_name in df_success['experiment'].unique():
            df_exp = df_success[df_success['experiment'] == exp_name]
            if len(df_exp) > 0 and 'test_auc' in df_exp.columns:
                best = df_exp.loc[df_exp['test_auc'].idxmax()]
                model_col = 'model' if 'model' in best else 'variant'
                print(f"  {exp_name}: {best.get(model_col, 'N/A')} - AUC={best['test_auc']:.4f}")
        
        print("=" * 80)
        print("âœ… DDP è®­ç»ƒå®Œæˆï¼")
    
    # æ¸…ç†
    cleanup_ddp()


# ========================================
# DDP æ•°æ®ä¸‹è½½è¾…åŠ©å‡½æ•°
# ========================================

def _ensure_data_downloaded(dataset_name, rank, world_size):
    """
    DDP å®‰å…¨çš„æ•°æ®ä¸‹è½½ï¼šåªæœ‰ rank 0 ä¸‹è½½ï¼Œå…¶ä»–è¿›ç¨‹ç­‰å¾…
    """
    import urllib.request
    import zipfile
    
    data_dir = './data'
    data_path = os.path.join(data_dir, dataset_name)
    
    # ç¡®å®šå…³é”®æ–‡ä»¶
    if dataset_name == 'ml-100k':
        key_file = os.path.join(data_path, 'u.data')
        url = 'https://files.grouplens.org/datasets/movielens/ml-100k.zip'
    elif dataset_name == 'ml-1m':
        key_file = os.path.join(data_path, 'ratings.dat')
        url = 'https://files.grouplens.org/datasets/movielens/ml-1m.zip'
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    # Rank 0 æ£€æŸ¥å¹¶ä¸‹è½½
    if is_main_process(rank):
        if os.path.exists(key_file):
            print(f"âœ… æ•°æ®å·²å­˜åœ¨: {data_path}")
        else:
            print(f"ğŸ“¥ Rank 0 å¼€å§‹ä¸‹è½½æ•°æ®é›† {dataset_name}...")
            os.makedirs(data_dir, exist_ok=True)
            
            zip_path = os.path.join(data_dir, f'{dataset_name}.zip')
            urllib.request.urlretrieve(url, zip_path)
            print(f"ğŸ“¦ è§£å‹æ•°æ®...")
            
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(data_dir)
            
            os.remove(zip_path)
            print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {data_path}")
    
    # æ‰€æœ‰è¿›ç¨‹åŒæ­¥ç­‰å¾…
    barrier()
    print_main(f"ğŸ”„ æ‰€æœ‰è¿›ç¨‹å·²åŒæ­¥ï¼Œæ•°æ®å¯ç”¨: {dataset_name}", rank)


# ========================================
# å®éªŒ1: åºåˆ—é•¿åº¦ + æ¨¡å‹å¯¹æ¯”
# ========================================

def run_experiment1(dataset_name, device, local_rank, world_size, rank,
                    EPOCHS, SEQ_LENGTHS, BATCH_SIZE_PER_GPU, EMBEDDING_DIM,
                    NUM_WORKERS, PREFETCH_FACTOR, MODELS_TO_TEST,
                    ENABLE_TOPK, TOPK_VALUES, NUM_NEG_SAMPLES,
                    TENSORBOARD_LOG_DIR):
    """å®éªŒ1: åºåˆ—é•¿åº¦ + æ¨¡å‹å¯¹æ¯”"""
    
    print_main("\n" + "=" * 60, rank)
    print_main("ğŸ“Š å®éªŒ1: åºåˆ—é•¿åº¦ + æ¨¡å‹å¯¹æ¯”", rank)
    print_main("=" * 60, rank)
    
    # DDP: åªåœ¨ rank 0 é¢„ä¸‹è½½æ•°æ®ï¼Œå…¶ä»–è¿›ç¨‹ç­‰å¾…
    _ensure_data_downloaded(dataset_name, rank, world_size)
    
    results = []
    
    for seq_length in SEQ_LENGTHS:
        print_main(f"\nğŸ”¬ åºåˆ—é•¿åº¦: {seq_length}", rank)
        
        # åˆ›å»ºç‰¹å¾å¤„ç†å™¨ï¼ˆç¡®ä¿æ•°æ®å·²å‡†å¤‡å¥½ï¼‰
        barrier()  # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
        fp = FeatureProcessor('./data', dataset_name)
        barrier()  # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆåˆå§‹åŒ–
        
        # åŠ è½½ Top-K è¯„ä¼°æ•°æ®ï¼ˆä»…åœ¨å¯ç”¨æ—¶ï¼‰
        eval_data, ie_eval = None, None
        if ENABLE_TOPK and is_main_process(rank):
            eval_data, _, fp_eval, ie_eval = get_topk_eval_data(
                data_dir='./data',
                dataset_name=dataset_name,
                max_seq_length=seq_length,
                num_neg_samples=NUM_NEG_SAMPLES
            )
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = RichFeatureDataset(
            data_dir='./data',
            dataset_name=dataset_name,
            max_seq_length=seq_length,
            split='train',
            feature_processor=fp
        )
        
        valid_dataset = RichFeatureDataset(
            data_dir='./data',
            dataset_name=dataset_name,
            max_seq_length=seq_length,
            split='valid',
            feature_processor=fp
        )
        
        test_dataset = RichFeatureDataset(
            data_dir='./data',
            dataset_name=dataset_name,
            max_seq_length=seq_length,
            split='test',
            feature_processor=fp
        )
        
        dataset_info = {
            'num_items': train_dataset.num_items,
            'num_users': train_dataset.num_users,
            'feature_dims': fp.get_feature_dims()
        }
        
        # DDP DataLoader
        train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)
        valid_sampler = DistributedSampler(valid_dataset, num_replicas=world_size, rank=rank, shuffle=False)
        test_sampler = DistributedSampler(test_dataset, num_replicas=world_size, rank=rank, shuffle=False)
        
        train_loader = DataLoader(
            train_dataset, batch_size=BATCH_SIZE_PER_GPU, sampler=train_sampler,
            num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=PREFETCH_FACTOR,
            persistent_workers=True if NUM_WORKERS > 0 else False
        )
        valid_loader = DataLoader(
            valid_dataset, batch_size=BATCH_SIZE_PER_GPU, sampler=valid_sampler,
            num_workers=NUM_WORKERS, pin_memory=True
        )
        test_loader = DataLoader(
            test_dataset, batch_size=BATCH_SIZE_PER_GPU, sampler=test_sampler,
            num_workers=NUM_WORKERS, pin_memory=True
        )
        
        # æµ‹è¯•å„æ¨¡å‹
        for model_name in MODELS_TO_TEST:
            print_main(f"  ğŸš€ {model_name}...", rank)
            
            try:
                # åˆ›å»ºæ¨¡å‹ï¼ˆæ‰€æœ‰è¿›ç¨‹éƒ½æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼‰
                print(f"    [Rank {rank}] æ­£åœ¨åˆ›å»ºæ¨¡å‹ {model_name}...")
                model = create_model(model_name, dataset_info, EMBEDDING_DIM, seq_length)
                print(f"    [Rank {rank}] æ¨¡å‹åˆ›å»ºå®Œæˆï¼Œç§»åŠ¨åˆ°è®¾å¤‡ {device}...")
                model = model.to(device)
                print(f"    [Rank {rank}] æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡")
                
                # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
                print(f"    [Rank {rank}] ç­‰å¾…barrieråŒæ­¥...")
                barrier()
                print(f"    [Rank {rank}] barrieråŒæ­¥å®Œæˆ")
                
                # DDP åŒ…è£…
                if world_size > 1:
                    print(f"    [Rank {rank}] æ­£åœ¨åŒ…è£… DDP...")
                    model = DDP(model, device_ids=[local_rank], output_device=local_rank)
                    print(f"    [Rank {rank}] DDP åŒ…è£…å®Œæˆ")
                
                # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
                print(f"    [Rank {rank}] ç­‰å¾…barrieråŒæ­¥...")
                barrier()
                print(f"    [Rank {rank}] barrieråŒæ­¥å®Œæˆï¼Œå‡†å¤‡åˆ›å»ºtrainer")
                
                # è®­ç»ƒ
                trainer = SimpleDDPTrainer(model, device, local_rank, rank, world_size,
                                          TENSORBOARD_LOG_DIR, f'exp1_{dataset_name}_{model_name}_seq{seq_length}',
                                          patience=7, grad_clip=1.0)
                
                t1 = time.time()
                early_stopped = False
                for epoch in range(EPOCHS):
                    train_sampler.set_epoch(epoch)  # DDP é‡è¦ï¼
                    train_loss = trainer.train_epoch(train_loader)
                    
                    if is_main_process(rank):
                        valid_metrics = trainer.evaluate(valid_loader)
                        # å­¦ä¹ ç‡è°ƒåº¦å’Œæ—©åœæ£€æŸ¥
                        early_stopped = trainer.step_scheduler(valid_metrics['auc'])
                        
                        if (epoch + 1) % 10 == 0 or epoch == EPOCHS - 1:
                            lr = trainer.optimizer.param_groups[0]['lr']
                            print(f"    Epoch {epoch+1}/{EPOCHS} - Loss: {train_loss:.4f} - Valid AUC: {valid_metrics['auc']:.4f} - LR: {lr:.2e}")
                    
                    # å¹¿æ’­æ—©åœä¿¡å·åˆ°æ‰€æœ‰è¿›ç¨‹
                    if world_size > 1:
                        early_stop_tensor = torch.tensor([1 if early_stopped else 0], device=device)
                        dist.broadcast(early_stop_tensor, src=0)
                        early_stopped = early_stop_tensor.item() == 1
                    
                    if early_stopped:
                        print_main(f"    â¹ï¸ æ—©åœè§¦å‘ @ epoch {epoch+1}", rank)
                        break
                
                train_time = time.time() - t1
                
                # æ¢å¤æœ€ä½³æ¨¡å‹
                if is_main_process(rank):
                    trainer.restore_best_model()
                
                # æµ‹è¯•
                if is_main_process(rank):
                    test_metrics = trainer.evaluate(test_loader)
                    
                    result = {
                        'experiment': 'exp1_model_comparison',
                        'dataset': dataset_name,
                        'seq_length': seq_length,
                        'model': model_name,
                        'test_auc': test_metrics['auc'],
                        'test_logloss': test_metrics['logloss'],
                        'train_time_sec': train_time,
                        'world_size': world_size,
                        'status': 'success'
                    }
                    
                    # Top-K è¯„ä¼°
                    if ENABLE_TOPK and eval_data is not None:
                        topk_metrics = trainer.evaluate_topk(
                            eval_data=eval_data,
                            feature_processor=fp,
                            interaction_extractor=ie_eval,
                            max_seq_length=seq_length,
                            ks=TOPK_VALUES
                        )
                        result.update(topk_metrics)
                        print(f"    âœ… AUC={test_metrics['auc']:.4f}, HR@10={topk_metrics['HR@10']:.4f}, NDCG@10={topk_metrics['NDCG@10']:.4f}, Time={train_time:.1f}s")
                    else:
                        print(f"    âœ… AUC={test_metrics['auc']:.4f}, LogLoss={test_metrics['logloss']:.4f}, Time={train_time:.1f}s")
                    
                    # è®°å½•è¶…å‚æ•°
                    trainer.log_hparams(
                        {'model': model_name, 'seq_length': seq_length, 'epochs': EPOCHS},
                        {'hparam/test_auc': test_metrics['auc']}
                    )
                    trainer.close()
                    
                    results.append(result)
                
                barrier()
                
            except Exception as e:
                print_main(f"    âŒ {str(e)[:100]}", rank)
                if is_main_process(rank):
                    results.append({
                        'experiment': 'exp1_model_comparison',
                        'dataset': dataset_name,
                        'seq_length': seq_length,
                        'model': model_name,
                        'status': f'error: {str(e)[:100]}'
                    })
    
    return results


# ========================================
# å®éªŒ2: æ–¹æ³•å¯¹æ¯”
# ========================================

def run_experiment2(dataset_name, device, local_rank, world_size, rank,
                    EPOCHS, BATCH_SIZE_PER_GPU, EMBEDDING_DIM,
                    NUM_WORKERS, PREFETCH_FACTOR,
                    TENSORBOARD_LOG_DIR,
                    ENABLE_TOPK=True, TOPK_VALUES=[5, 10, 20]):
    """å®éªŒ2: æ–¹æ³•å¯¹æ¯”ï¼ˆDIN vs ä¼ ç»Ÿæ–¹æ³•ï¼‰"""
    
    print_main("\n" + "=" * 60, rank)
    print_main("ğŸ“Š å®éªŒ2: DIN vs ä¼ ç»Ÿæ–¹æ³•", rank)
    print_main("=" * 60, rank)
    
    # DDP: é¢„ä¸‹è½½æ•°æ®
    _ensure_data_downloaded(dataset_name, rank, world_size)
    
    results = []
    seq_length = 50  # å›ºå®šåºåˆ—é•¿åº¦
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆç¡®ä¿æ•°æ®å·²å‡†å¤‡å¥½ï¼‰
    barrier()  # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
    fp = FeatureProcessor('./data', dataset_name)
    barrier()  # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆåˆå§‹åŒ–
    
    # Top-K è¯„ä¼°æ•°æ®
    eval_data = None
    ie_eval = None
    if ENABLE_TOPK and is_main_process(rank):
        try:
            eval_data, _, fp_eval, ie_eval = get_topk_eval_data('./data', dataset_name, seq_length)
            print_main(f"  ğŸ“Š Top-Kè¯„ä¼°æ•°æ®åŠ è½½å®Œæˆ: {len(eval_data)} æ¡", rank)
        except Exception as e:
            print_main(f"  âš ï¸ Top-Kè¯„ä¼°æ•°æ®åŠ è½½å¤±è´¥: {e}", rank)
            ENABLE_TOPK = False
    
    train_dataset = RichFeatureDataset(
        data_dir='./data',
        dataset_name=dataset_name,
        max_seq_length=seq_length,
        split='train',
        feature_processor=fp
    )
    valid_dataset = RichFeatureDataset(
        data_dir='./data',
        dataset_name=dataset_name,
        max_seq_length=seq_length,
        split='valid',
        feature_processor=fp
    )
    test_dataset = RichFeatureDataset(
        data_dir='./data',
        dataset_name=dataset_name,
        max_seq_length=seq_length,
        split='test',
        feature_processor=fp
    )
    
    dataset_info = {
        'num_items': train_dataset.num_items,
        'num_users': train_dataset.num_users,
        'feature_dims': fp.get_feature_dims()
    }
    
    # DDP DataLoader
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)
    
    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE_PER_GPU, sampler=train_sampler,
        num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=PREFETCH_FACTOR,
        persistent_workers=True if NUM_WORKERS > 0 else False
    )
    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE_PER_GPU, num_workers=NUM_WORKERS, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE_PER_GPU, num_workers=NUM_WORKERS, pin_memory=True)
    
    methods = ['DIN', 'AvgPool']
    
    for method in methods:
        print_main(f"  ğŸš€ {method}...", rank)
        
        try:
            model = create_model(method, dataset_info, EMBEDDING_DIM, seq_length)
            model = model.to(device)
            
            if world_size > 1:
                model = DDP(model, device_ids=[local_rank], output_device=local_rank)
            
            trainer = SimpleDDPTrainer(model, device, local_rank, rank, world_size,
                                      TENSORBOARD_LOG_DIR, f'exp2_{dataset_name}_{method}',
                                      patience=7, grad_clip=1.0)
            
            t1 = time.time()
            early_stopped = False
            for epoch in range(EPOCHS):
                train_sampler.set_epoch(epoch)
                train_loss = trainer.train_epoch(train_loader)
                
                if is_main_process(rank):
                    valid_metrics = trainer.evaluate(valid_loader)
                    early_stopped = trainer.step_scheduler(valid_metrics['auc'])
                    
                    if (epoch + 1) % 10 == 0:
                        lr = trainer.optimizer.param_groups[0]['lr']
                        print(f"    Epoch {epoch+1}/{EPOCHS} - Loss: {train_loss:.4f} - Valid AUC: {valid_metrics['auc']:.4f} - LR: {lr:.2e}")
                
                # å¹¿æ’­æ—©åœä¿¡å·
                if world_size > 1:
                    early_stop_tensor = torch.tensor([1 if early_stopped else 0], device=device)
                    dist.broadcast(early_stop_tensor, src=0)
                    early_stopped = early_stop_tensor.item() == 1
                
                if early_stopped:
                    print_main(f"    â¹ï¸ æ—©åœè§¦å‘ @ epoch {epoch+1}", rank)
                    break
            
            train_time = time.time() - t1
            
            # æ¢å¤æœ€ä½³æ¨¡å‹
            if is_main_process(rank):
                trainer.restore_best_model()
            
            if is_main_process(rank):
                test_metrics = trainer.evaluate(test_loader)
                
                result = {
                    'experiment': 'exp2_method_comparison',
                    'dataset': dataset_name,
                    'method': method,
                    'test_auc': test_metrics['auc'],
                    'test_logloss': test_metrics['logloss'],
                    'train_time_sec': train_time,
                    'status': 'success'
                }
                
                # Top-K è¯„ä¼°
                if ENABLE_TOPK and eval_data is not None:
                    topk_metrics = trainer.evaluate_topk(
                        eval_data=eval_data,
                        feature_processor=fp,
                        interaction_extractor=ie_eval,
                        max_seq_length=seq_length,
                        ks=TOPK_VALUES
                    )
                    result.update(topk_metrics)
                    print(f"    âœ… AUC={test_metrics['auc']:.4f}, HR@10={topk_metrics['HR@10']:.4f}, NDCG@10={topk_metrics['NDCG@10']:.4f}")
                else:
                    print(f"    âœ… AUC={test_metrics['auc']:.4f}")
                
                # è®°å½•è¶…å‚æ•°å’Œå…³é—­
                trainer.log_hparams(
                    {'method': method, 'epochs': EPOCHS},
                    {'hparam/test_auc': test_metrics['auc']}
                )
                trainer.close()
                
                results.append(result)
            
            barrier()
            
        except Exception as e:
            print_main(f"    âŒ {str(e)[:100]}", rank)
            if is_main_process(rank):
                results.append({
                    'experiment': 'exp2_method_comparison',
                    'dataset': dataset_name,
                    'method': method,
                    'status': f'error: {str(e)[:100]}'
                })
    
    return results


# ========================================
# å®éªŒ3: æ¶ˆèå®éªŒ
# ========================================

def run_experiment3(dataset_name, device, local_rank, world_size, rank,
                    EPOCHS, BATCH_SIZE_PER_GPU, EMBEDDING_DIM,
                    NUM_WORKERS, PREFETCH_FACTOR,
                    TENSORBOARD_LOG_DIR,
                    ENABLE_TOPK=True, TOPK_VALUES=[5, 10, 20]):
    """å®éªŒ3: æ¶ˆèå®éªŒ"""
    
    print_main("\n" + "=" * 60, rank)
    print_main("ğŸ“Š å®éªŒ3: æ¶ˆèå®éªŒ", rank)
    print_main("=" * 60, rank)
    
    # DDP: é¢„ä¸‹è½½æ•°æ®
    _ensure_data_downloaded(dataset_name, rank, world_size)
    
    results = []
    seq_length = 50
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆç¡®ä¿æ•°æ®å·²å‡†å¤‡å¥½ï¼‰
    barrier()  # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
    fp = FeatureProcessor('./data', dataset_name)
    barrier()  # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆåˆå§‹åŒ–
    
    # Top-K è¯„ä¼°æ•°æ®
    eval_data = None
    ie_eval = None
    if ENABLE_TOPK and is_main_process(rank):
        try:
            eval_data, _, fp_eval, ie_eval = get_topk_eval_data('./data', dataset_name, seq_length)
            print_main(f"  ğŸ“Š Top-Kè¯„ä¼°æ•°æ®åŠ è½½å®Œæˆ: {len(eval_data)} æ¡", rank)
        except Exception as e:
            print_main(f"  âš ï¸ Top-Kè¯„ä¼°æ•°æ®åŠ è½½å¤±è´¥: {e}", rank)
            ENABLE_TOPK = False
    
    train_dataset = RichFeatureDataset(
        data_dir='./data',
        dataset_name=dataset_name,
        max_seq_length=seq_length,
        split='train',
        feature_processor=fp
    )
    valid_dataset = RichFeatureDataset(
        data_dir='./data',
        dataset_name=dataset_name,
        max_seq_length=seq_length,
        split='valid',
        feature_processor=fp
    )
    test_dataset = RichFeatureDataset(
        data_dir='./data',
        dataset_name=dataset_name,
        max_seq_length=seq_length,
        split='test',
        feature_processor=fp
    )
    
    dataset_info = {
        'num_items': train_dataset.num_items,
        'num_users': train_dataset.num_users,
        'feature_dims': fp.get_feature_dims()
    }
    
    # DDP DataLoader
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)
    
    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE_PER_GPU, sampler=train_sampler,
        num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=PREFETCH_FACTOR,
        persistent_workers=True if NUM_WORKERS > 0 else False
    )
    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE_PER_GPU, num_workers=NUM_WORKERS, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE_PER_GPU, num_workers=NUM_WORKERS, pin_memory=True)
    
    # æ¶ˆèå˜ä½“
    variants = ['full_din', 'no_attention', 'shallow_mlp']
    
    for variant in variants:
        print_main(f"  ğŸš€ {variant}...", rank)
        
        try:
            model = create_ablation_model(variant, dataset_info, EMBEDDING_DIM)
            model = model.to(device)
            
            if world_size > 1:
                model = DDP(model, device_ids=[local_rank], output_device=local_rank)
            
            trainer = SimpleDDPTrainer(model, device, local_rank, rank, world_size,
                                      TENSORBOARD_LOG_DIR, f'exp3_{dataset_name}_{variant}',
                                      patience=7, grad_clip=1.0)
            
            t1 = time.time()
            early_stopped = False
            for epoch in range(EPOCHS):
                train_sampler.set_epoch(epoch)
                train_loss = trainer.train_epoch(train_loader)
                
                if is_main_process(rank):
                    valid_metrics = trainer.evaluate(valid_loader)
                    early_stopped = trainer.step_scheduler(valid_metrics['auc'])
                    
                    if (epoch + 1) % 10 == 0:
                        lr = trainer.optimizer.param_groups[0]['lr']
                        print(f"    Epoch {epoch+1}/{EPOCHS} - Loss: {train_loss:.4f} - Valid AUC: {valid_metrics['auc']:.4f} - LR: {lr:.2e}")
                
                # å¹¿æ’­æ—©åœä¿¡å·
                if world_size > 1:
                    early_stop_tensor = torch.tensor([1 if early_stopped else 0], device=device)
                    dist.broadcast(early_stop_tensor, src=0)
                    early_stopped = early_stop_tensor.item() == 1
                
                if early_stopped:
                    print_main(f"    â¹ï¸ æ—©åœè§¦å‘ @ epoch {epoch+1}", rank)
                    break
            
            train_time = time.time() - t1
            
            # æ¢å¤æœ€ä½³æ¨¡å‹
            if is_main_process(rank):
                trainer.restore_best_model()
            
            if is_main_process(rank):
                test_metrics = trainer.evaluate(test_loader)
                
                result = {
                    'experiment': 'exp3_ablation',
                    'dataset': dataset_name,
                    'variant': variant,
                    'test_auc': test_metrics['auc'],
                    'test_logloss': test_metrics['logloss'],
                    'train_time_sec': train_time,
                    'status': 'success'
                }
                
                # Top-K è¯„ä¼°
                if ENABLE_TOPK and eval_data is not None:
                    topk_metrics = trainer.evaluate_topk(
                        eval_data=eval_data,
                        feature_processor=fp,
                        interaction_extractor=ie_eval,
                        max_seq_length=seq_length,
                        ks=TOPK_VALUES
                    )
                    result.update(topk_metrics)
                    print(f"    âœ… AUC={test_metrics['auc']:.4f}, HR@10={topk_metrics['HR@10']:.4f}, NDCG@10={topk_metrics['NDCG@10']:.4f}")
                else:
                    print(f"    âœ… AUC={test_metrics['auc']:.4f}")
                
                # è®°å½•è¶…å‚æ•°å’Œå…³é—­
                trainer.log_hparams(
                    {'variant': variant, 'epochs': EPOCHS},
                    {'hparam/test_auc': test_metrics['auc']}
                )
                trainer.close()
                
                results.append(result)
            
            barrier()
            
        except Exception as e:
            print_main(f"    âŒ {str(e)[:100]}", rank)
            if is_main_process(rank):
                results.append({
                    'experiment': 'exp3_ablation',
                    'dataset': dataset_name,
                    'variant': variant,
                    'status': f'error: {str(e)[:100]}'
                })
    
    return results


def run_experiment4(dataset_name, device, local_rank, world_size, rank,
                    EPOCHS, BATCH_SIZE_PER_GPU, EMBEDDING_DIM,
                    NUM_WORKERS, PREFETCH_FACTOR,
                    TENSORBOARD_LOG_DIR, part='all'):
    """
    å®éªŒ4: é«˜çº§æ”¹è¿›å®éªŒï¼ˆè‡ªé€‚åº”æ—¶é—´è¡°å‡ + å¯¹æ¯”å­¦ä¹ ï¼‰
    
    DDP ç‰ˆæœ¬ï¼šé€šè¿‡åŠ¨æ€å¯¼å…¥ experiment4.py å®ç°
    ä»…åœ¨ä¸»è¿›ç¨‹ä¸Šè¿è¡Œï¼ˆå› ä¸º experiment4.py å†…éƒ¨ä¸æ”¯æŒ DDPï¼‰
    """
    
    print_main("\n" + "=" * 60, rank)
    print_main("ğŸ“Š å®éªŒ4: é«˜çº§æ”¹è¿›å®éªŒ", rank)
    print_main("=" * 60, rank)
    
    results = []
    
    # å®éªŒ4 çš„åŸå§‹å®ç°ä¸æ”¯æŒ DDPï¼Œä»…åœ¨ä¸»è¿›ç¨‹è¿è¡Œ
    # éä¸»è¿›ç¨‹ç›´æ¥è·³è¿‡å®éªŒï¼Œæœ€åç»Ÿä¸€ barrier
    if not is_main_process(rank):
        # éä¸»è¿›ç¨‹ç­‰å¾…ä¸»è¿›ç¨‹å®Œæˆåå†åŒæ­¥
        barrier()
        return results
    
    # === ä¸»è¿›ç¨‹æ‰§è¡Œå®éªŒ ===
    try:
        # åŠ¨æ€å¯¼å…¥ experiment4 æ¨¡å—
        import importlib.util
        exp4_path = os.path.join(os.path.dirname(__file__), 'experiment4.py')
        
        if not os.path.exists(exp4_path):
            print("âŒ experiment4.py ä¸å­˜åœ¨ï¼Œè·³è¿‡å®éªŒå››")
            # ä¸»è¿›ç¨‹ä¹Ÿè¦è°ƒç”¨ barrier ä»¥åŒ¹é…éä¸»è¿›ç¨‹
            barrier()
            return results
        
        spec = importlib.util.spec_from_file_location("experiment4", exp4_path)
        exp4_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(exp4_module)
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ª GPU è¿è¡Œï¼ˆä¸»è¿›ç¨‹ï¼‰
        exp4_device = f'cuda:{local_rank}' if torch.cuda.is_available() else 'cpu'
        
        # Part 1: è‡ªé€‚åº”æ—¶é—´è¡°å‡å®éªŒ
        if part in ['all', 'adaptive']:
            print("\nğŸ“Š Part 1: è‡ªé€‚åº”æ—¶é—´è¡°å‡å®éªŒ")
            print("-" * 40)
            try:
                adaptive_results = exp4_module.run_adaptive_decay_experiment(
                    dataset_name=dataset_name,
                    epochs=EPOCHS,
                    batch_size=BATCH_SIZE_PER_GPU * world_size,  # ä½¿ç”¨å®Œæ•´ batch size
                    device=exp4_device
                )
                for r in adaptive_results:
                    r['experiment'] = 'exp4_adaptive_decay'
                    r['dataset'] = dataset_name
                results.extend(adaptive_results)
                print(f"âœ… è‡ªé€‚åº”æ—¶é—´è¡°å‡å®éªŒå®Œæˆï¼Œ{len(adaptive_results)} ç»„ç»“æœ")
            except Exception as e:
                print(f"âŒ è‡ªé€‚åº”æ—¶é—´è¡°å‡å®éªŒå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                results.append({
                    'experiment': 'exp4_adaptive_decay',
                    'dataset': dataset_name,
                    'status': f'error: {str(e)[:100]}'
                })
        
        # Part 2: å¯¹æ¯”å­¦ä¹ å®éªŒ
        if part in ['all', 'contrastive']:
            print("\nğŸ“Š Part 2: å¯¹æ¯”å­¦ä¹ å®éªŒ")
            print("-" * 40)
            try:
                contrastive_results = exp4_module.run_contrastive_experiment(
                    dataset_name=dataset_name,
                    epochs=EPOCHS,
                    batch_size=BATCH_SIZE_PER_GPU * world_size,
                    device=exp4_device
                )
                for r in contrastive_results:
                    r['experiment'] = 'exp4_contrastive'
                    r['dataset'] = dataset_name
                results.extend(contrastive_results)
                print(f"âœ… å¯¹æ¯”å­¦ä¹ å®éªŒå®Œæˆï¼Œ{len(contrastive_results)} ç»„ç»“æœ")
            except Exception as e:
                print(f"âŒ å¯¹æ¯”å­¦ä¹ å®éªŒå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                results.append({
                    'experiment': 'exp4_contrastive',
                    'dataset': dataset_name,
                    'status': f'error: {str(e)[:100]}'
                })
                
    except Exception as e:
        print(f"âŒ å®éªŒå››åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        results.append({
            'experiment': 'exp4',
            'dataset': dataset_name,
            'status': f'load_error: {str(e)[:100]}'
        })
    
    barrier()  # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
    return results


# ========================================
# å·¥å…·ç±»å’Œå‡½æ•°
# ========================================

def create_model(model_name, dataset_info, embedding_dim, seq_length):
    """åˆ›å»ºæ¨¡å‹"""
    if model_name == 'DIN':
        return DINRichLite(
            num_items=dataset_info['num_items'],
            num_users=dataset_info['num_users'],
            feature_dims=dataset_info['feature_dims'],
            embedding_dim=embedding_dim
        )
    elif model_name == 'GRU4Rec':
        return GRU4Rec(
            num_items=dataset_info['num_items'],
            num_users=dataset_info['num_users'],
            feature_dims=dataset_info['feature_dims'],
            embedding_dim=embedding_dim,
            hidden_dim=embedding_dim
        )
    elif model_name == 'SASRec':
        return SASRec(
            num_items=dataset_info['num_items'],
            num_users=dataset_info['num_users'],
            feature_dims=dataset_info['feature_dims'],
            embedding_dim=embedding_dim,
            num_heads=2,
            num_layers=2,
            max_seq_len=seq_length
        )
    elif model_name == 'NARM':
        return NARM(
            num_items=dataset_info['num_items'],
            num_users=dataset_info['num_users'],
            feature_dims=dataset_info['feature_dims'],
            embedding_dim=embedding_dim,
            hidden_dim=embedding_dim
        )
    elif model_name == 'AvgPool':
        return SimpleAveragePoolingRich(
            num_items=dataset_info['num_items'],
            num_users=dataset_info['num_users'],
            feature_dims=dataset_info['feature_dims'],
            embedding_dim=embedding_dim
        )
    else:
        raise ValueError(f"Unknown model: {model_name}")


def create_ablation_model(variant, dataset_info, embedding_dim):
    """åˆ›å»ºæ¶ˆèå®éªŒæ¨¡å‹"""
    if variant == 'full_din':
        return DINRichLite(
            num_items=dataset_info['num_items'],
            num_users=dataset_info['num_users'],
            feature_dims=dataset_info['feature_dims'],
            embedding_dim=embedding_dim
        )
    elif variant == 'no_attention':
        return SimpleAveragePoolingRich(
            num_items=dataset_info['num_items'],
            num_users=dataset_info['num_users'],
            feature_dims=dataset_info['feature_dims'],
            embedding_dim=embedding_dim
        )
    elif variant == 'shallow_mlp':
        return DINRichLite(
            num_items=dataset_info['num_items'],
            num_users=dataset_info['num_users'],
            feature_dims=dataset_info['feature_dims'],
            embedding_dim=embedding_dim,
            mlp_hidden_dims=[128, 64]  # æ›´æµ…çš„ MLP
        )
    else:
        raise ValueError(f"Unknown variant: {variant}")


class SimpleDDPTrainer:
    """
    å®Œæ•´çš„ DDP è®­ç»ƒå™¨
    
    åŒ…å«ï¼š
    - å­¦ä¹ ç‡è°ƒåº¦å™¨ (ReduceLROnPlateau)
    - æ—©åœæœºåˆ¶ (Early Stopping)
    - æ¢¯åº¦è£å‰ª (Gradient Clipping)
    - æ··åˆç²¾åº¦è®­ç»ƒ (AMP)
    - TensorBoard æ—¥å¿—
    """
    
    def __init__(self, model, device, local_rank, rank, world_size, log_dir, exp_name,
                 learning_rate=1e-3, weight_decay=1e-5, 
                 patience=5, grad_clip=1.0,
                 lr_scheduler_patience=3, lr_scheduler_factor=0.5):
        self.model = model
        self.device = device
        self.local_rank = local_rank
        self.rank = rank
        self.world_size = world_size
        self.is_main = (rank == 0)
        
        # è®­ç»ƒé…ç½®
        self.patience = patience  # æ—©åœè€å¿ƒå€¼
        self.grad_clip = grad_clip  # æ¢¯åº¦è£å‰ªé˜ˆå€¼
        
        self.criterion = nn.BCEWithLogitsLoss()
        
        # è·å–åŸå§‹æ¨¡å‹å‚æ•°
        if hasattr(model, 'module'):
            params = model.module.parameters()
        else:
            params = model.parameters()
        
        self.optimizer = torch.optim.Adam(params, lr=learning_rate, weight_decay=weight_decay)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ (ReduceLROnPlateau)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', factor=lr_scheduler_factor, 
            patience=lr_scheduler_patience, verbose=False
        )
        
        # AMP
        self.scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None
        
        # æ—©åœçŠ¶æ€
        self.best_valid_auc = 0.0
        self.best_epoch = 0
        self.epochs_without_improvement = 0
        self.best_model_state = None
        
        # TensorBoardï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        self.writer = None
        if self.is_main and HAS_TENSORBOARD:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            tb_dir = os.path.join(log_dir, f"{exp_name}_{timestamp}")
            os.makedirs(tb_dir, exist_ok=True)
            self.writer = SummaryWriter(tb_dir)
        
        self.epoch = 0
        self.global_step = 0
    
    def _move_batch(self, batch):
        return {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}
    
    def _get_raw_model(self):
        """è·å–åŸå§‹æ¨¡å‹ï¼ˆDDPåŒ…è£…ä¸‹ï¼‰"""
        if hasattr(self.model, 'module'):
            return self.model.module
        return self.model
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ª epochï¼ˆå«æ¢¯åº¦è£å‰ªï¼‰"""
        self.model.train()
        total_loss = 0
        
        for batch in train_loader:
            batch = self._move_batch(batch)
            self.optimizer.zero_grad()
            
            if self.scaler:
                with torch.cuda.amp.autocast():
                    logits = self.model(batch)
                    loss = self.criterion(logits, batch['label'])
                self.scaler.scale(loss).backward()
                # æ¢¯åº¦è£å‰ªï¼ˆAMPæ¨¡å¼ä¸‹éœ€è¦å…ˆ unscaleï¼‰
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self._get_raw_model().parameters(), self.grad_clip)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                logits = self.model(batch)
                loss = self.criterion(logits, batch['label'])
                loss.backward()
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self._get_raw_model().parameters(), self.grad_clip)
                self.optimizer.step()
            
            total_loss += loss.item()
            self.global_step += 1
        
        avg_loss = total_loss / len(train_loader)
        
        if self.writer:
            self.writer.add_scalar('Loss/train', avg_loss, self.epoch)
            self.writer.add_scalar('LR/learning_rate', self.optimizer.param_groups[0]['lr'], self.epoch)
        
        self.epoch += 1
        return avg_loss
    
    def step_scheduler(self, valid_auc):
        """æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨å’Œæ—©åœæ£€æŸ¥"""
        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler.step(valid_auc)
        
        # æ—©åœæ£€æŸ¥
        if valid_auc > self.best_valid_auc:
            self.best_valid_auc = valid_auc
            self.best_epoch = self.epoch
            self.epochs_without_improvement = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
            self.best_model_state = {k: v.cpu().clone() for k, v in self._get_raw_model().state_dict().items()}
            return False  # ä¸æ—©åœ
        else:
            self.epochs_without_improvement += 1
            if self.epochs_without_improvement >= self.patience:
                return True  # è§¦å‘æ—©åœ
            return False
    
    def restore_best_model(self):
        """æ¢å¤æœ€ä½³æ¨¡å‹"""
        if self.best_model_state is not None:
            self._get_raw_model().load_state_dict(self.best_model_state)
            if self.is_main:
                print(f"    ğŸ“Œ æ¢å¤åˆ°æœ€ä½³æ¨¡å‹ (epoch {self.best_epoch}, AUC={self.best_valid_auc:.4f})")
    
    def evaluate(self, data_loader):
        self.model.eval()
        all_preds, all_labels = [], []
        
        with torch.no_grad():
            for batch in data_loader:
                batch = self._move_batch(batch)
                
                if self.scaler:
                    with torch.cuda.amp.autocast():
                        logits = self.model(batch)
                else:
                    logits = self.model(batch)
                
                preds = torch.sigmoid(logits).cpu().numpy()
                labels = batch['label'].cpu().numpy()
                all_preds.extend(preds)
                all_labels.extend(labels)
        
        from sklearn.metrics import roc_auc_score, log_loss
        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)
        
        auc = roc_auc_score(all_labels, all_preds)
        logloss = log_loss(all_labels, np.clip(all_preds, 1e-7, 1-1e-7))
        
        if self.writer:
            self.writer.add_scalar('Metrics/valid_auc', auc, self.epoch)
            self.writer.add_scalar('Metrics/valid_logloss', logloss, self.epoch)
        
        return {'auc': auc, 'logloss': logloss}
    
    def evaluate_topk(self, eval_data, feature_processor, interaction_extractor, 
                      max_seq_length, ks=[5, 10, 20]):
        """
        Top-K æ¨èè¯„ä¼°
        
        Args:
            eval_data: list of dictï¼Œæ¥è‡ª get_topk_eval_data
            feature_processor: ç‰¹å¾å¤„ç†å™¨
            interaction_extractor: äº¤äº’ç‰¹å¾æå–å™¨
            max_seq_length: æœ€å¤§åºåˆ—é•¿åº¦
            ks: è¯„ä¼°çš„ K å€¼åˆ—è¡¨
        
        Returns:
            dict: å„æŒ‡æ ‡åœ¨ä¸åŒ K ä¸‹çš„å€¼
        """
        self.model.eval()
        
        # åˆå§‹åŒ–æŒ‡æ ‡ç´¯åŠ å™¨
        all_hr = {k: [] for k in ks}
        all_ndcg = {k: [] for k in ks}
        all_mrr = {k: [] for k in ks}
        all_precision = {k: [] for k in ks}
        
        with torch.no_grad():
            for eval_item in eval_data:
                # æ„å»ºå•ç”¨æˆ·çš„å€™é€‰ batch
                batch = build_topk_batch_multi(
                    eval_item, feature_processor, interaction_extractor,
                    max_seq_length, self.device
                )
                
                # é¢„æµ‹åˆ†æ•°
                if self.scaler:
                    with torch.cuda.amp.autocast():
                        logits = self.model(batch)
                else:
                    logits = self.model(batch)
                
                scores = torch.sigmoid(logits).cpu().numpy()
                
                # æ’åº
                candidates = eval_item['candidates']
                ground_truth = eval_item['ground_truth']
                sorted_indices = np.argsort(-scores)
                ranked_items = [candidates[i] for i in sorted_indices]
                
                # è®¡ç®—æŒ‡æ ‡
                for k in ks:
                    all_hr[k].append(hit_at_k(ranked_items, ground_truth, k))
                    all_ndcg[k].append(ndcg_at_k(ranked_items, ground_truth, k))
                    all_mrr[k].append(mrr_at_k(ranked_items, ground_truth, k))
                    all_precision[k].append(precision_at_k(ranked_items, ground_truth, k))
        
        # è®¡ç®—å¹³å‡å€¼
        results = {}
        for k in ks:
            results[f'HR@{k}'] = np.mean(all_hr[k])
            results[f'Recall@{k}'] = np.mean(all_hr[k])  # å• GT ç­‰äº HR
            results[f'NDCG@{k}'] = np.mean(all_ndcg[k])
            results[f'MRR@{k}'] = np.mean(all_mrr[k])
            results[f'Precision@{k}'] = np.mean(all_precision[k])
        
        # è®°å½•åˆ° TensorBoard
        if self.writer:
            for k in ks:
                self.writer.add_scalar(f'TopK/HR@{k}', results[f'HR@{k}'], self.epoch)
                self.writer.add_scalar(f'TopK/NDCG@{k}', results[f'NDCG@{k}'], self.epoch)
                self.writer.add_scalar(f'TopK/MRR@{k}', results[f'MRR@{k}'], self.epoch)
                self.writer.add_scalar(f'TopK/Precision@{k}', results[f'Precision@{k}'], self.epoch)
        
        return results
    
    def log_hparams(self, hparams, metrics):
        """è®°å½•è¶…å‚æ•°å’Œæœ€ç»ˆæŒ‡æ ‡åˆ° TensorBoard"""
        if self.writer:
            self.writer.add_hparams(hparams, metrics)
    
    def close(self):
        """å…³é—­ TensorBoard writer"""
        if self.writer:
            self.writer.close()
            self.writer = None
    
    def __del__(self):
        self.close()


# ========================================
# å…¥å£
# ========================================

if __name__ == '__main__':
    main()
