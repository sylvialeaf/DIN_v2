#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å®éªŒä¸‰ï¼ˆå¢å¼ºç‰ˆï¼‰ï¼šDIN æ”¹è¿›æ¶ˆèå®éªŒ

åœ¨ä¸°å¯Œç‰¹å¾åŸºç¡€ä¸Šï¼Œæµ‹è¯•ä¸åŒæ”¹è¿›ç­–ç•¥çš„æ•ˆæœã€‚

æ¶ˆèå˜ä½“ï¼š
1. DIN-Rich-Base: åŸºç¡€ä¸°å¯Œç‰¹å¾ DIN
2. DIN-Rich-TimeDec: + æ—¶é—´è¡°å‡æ³¨æ„åŠ›
3. DIN-Rich-MultiHead: + å¤šå¤´æ³¨æ„åŠ›
4. DIN-Rich-Full: å®Œæ•´æ”¹è¿›
5. DIN-Rich-Full-v2: å®Œæ•´æ”¹è¿› + å¢å¼º MLP

ç‰¹å¾å·¥ç¨‹ï¼š
- ç”¨æˆ·ç”»åƒç‰¹å¾
- ç‰©å“å±æ€§ç‰¹å¾
- å†å²åºåˆ—ç‰¹å¾
- æ—¶é—´ä¸Šä¸‹æ–‡ç‰¹å¾

è¾“å‡º:
- results/experiment3_rich_results.csv
- results/experiment3_rich_plot.png
- results/experiment3_rich_report.json
"""

import os
import sys
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
import json
import time

from data_loader import get_rich_dataloaders
from trainer import RichTrainer, measure_inference_speed_rich
from models import AttentionLayer


# ========================================
# æ”¹è¿›ç‰ˆæ³¨æ„åŠ›å±‚ï¼ˆæ”¯æŒä¸°å¯Œç‰¹å¾ï¼‰
# ========================================

class TimeDecayRichAttention(nn.Module):
    """
    æ—¶é—´è¡°å‡ + ä¸°å¯Œç‰¹å¾æ³¨æ„åŠ›
    
    è¿‘æœŸè¡Œä¸ºæƒé‡æ›´é«˜ï¼Œç¬¦åˆå…´è¶£æ¼‚ç§»è§„å¾‹ã€‚
    """
    
    def __init__(self, input_dim, hidden_dims=[64, 32], time_decay=0.1):
        super(TimeDecayRichAttention, self).__init__()
        
        self.time_decay = time_decay
        
        mlp_input = 4 * input_dim
        layers = []
        prev_dim = mlp_input
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.PReLU())
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, 1))
        
        self.attention_mlp = nn.Sequential(*layers)
    
    def forward(self, query, keys, keys_mask=None):
        batch_size, seq_len, dim = keys.shape
        
        query_expanded = query.unsqueeze(1).expand(-1, seq_len, -1)
        
        attention_input = torch.cat([
            keys, query_expanded,
            keys * query_expanded,
            keys - query_expanded
        ], dim=-1)
        
        attention_scores = self.attention_mlp(attention_input).squeeze(-1)
        
        # æ—¶é—´è¡°å‡ï¼šä½ç½®è¶Šé åï¼ˆè¶Šè¿‘ï¼‰ï¼Œæƒé‡è¶Šå¤§
        positions = torch.arange(seq_len, device=keys.device).float()
        time_weights = torch.exp(self.time_decay * (positions - seq_len + 1))
        attention_scores = attention_scores * time_weights.unsqueeze(0)
        
        if keys_mask is not None:
            attention_scores = attention_scores.masked_fill(~keys_mask.bool(), -1e9)
        
        attention_weights = F.softmax(attention_scores, dim=-1)
        weighted_sum = torch.sum(attention_weights.unsqueeze(-1) * keys, dim=1)
        
        return weighted_sum, attention_weights


class MultiHeadRichAttention(nn.Module):
    """
    å¤šå¤´æ³¨æ„åŠ› + ä¸°å¯Œç‰¹å¾
    
    æ•è·ç”¨æˆ·çš„å¤šç»´å…´è¶£ã€‚
    """
    
    def __init__(self, input_dim, num_heads=4, hidden_dims=[64, 32]):
        super(MultiHeadRichAttention, self).__init__()
        
        self.num_heads = num_heads
        
        self.attention_heads = nn.ModuleList([
            self._build_attention_mlp(4 * input_dim, hidden_dims)
            for _ in range(num_heads)
        ])
        
        self.output_proj = nn.Linear(input_dim, input_dim)
    
    def _build_attention_mlp(self, input_dim, hidden_dims):
        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.PReLU())
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, 1))
        return nn.Sequential(*layers)
    
    def forward(self, query, keys, keys_mask=None):
        batch_size, seq_len, dim = keys.shape
        
        query_expanded = query.unsqueeze(1).expand(-1, seq_len, -1)
        
        attention_input = torch.cat([
            keys, query_expanded,
            keys * query_expanded,
            keys - query_expanded
        ], dim=-1)
        
        head_outputs = []
        all_weights = []  # æ”¶é›†æ‰€æœ‰å¤´çš„æ³¨æ„åŠ›æƒé‡
        for head in self.attention_heads:
            scores = head(attention_input).squeeze(-1)
            
            if keys_mask is not None:
                scores = scores.masked_fill(~keys_mask.bool(), -1e9)
            
            weights = F.softmax(scores, dim=-1)
            all_weights.append(weights)
            output = torch.sum(weights.unsqueeze(-1) * keys, dim=1)
            head_outputs.append(output)
        
        combined = torch.stack(head_outputs, dim=1).mean(dim=1)
        output = self.output_proj(combined)
        
        # è¿”å›å¹³å‡æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºå¯è§†åŒ–å’Œhybrid_rankerï¼‰
        avg_weights = torch.stack(all_weights, dim=1).mean(dim=1)  # [B, seq_len]
        return output, avg_weights


class TimeDecayMultiHeadRichAttention(nn.Module):
    """
    æ—¶é—´è¡°å‡ + å¤šå¤´æ³¨æ„åŠ›ï¼ˆå®Œæ•´æ”¹è¿›ï¼‰
    """
    
    def __init__(self, input_dim, num_heads=4, hidden_dims=[64, 32], time_decay=0.1):
        super(TimeDecayMultiHeadRichAttention, self).__init__()
        
        self.num_heads = num_heads
        self.time_decay = time_decay
        
        self.attention_heads = nn.ModuleList([
            self._build_attention_mlp(4 * input_dim, hidden_dims)
            for _ in range(num_heads)
        ])
        
        self.output_proj = nn.Linear(input_dim, input_dim)
    
    def _build_attention_mlp(self, input_dim, hidden_dims):
        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.PReLU())
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, 1))
        return nn.Sequential(*layers)
    
    def forward(self, query, keys, keys_mask=None):
        batch_size, seq_len, dim = keys.shape
        
        query_expanded = query.unsqueeze(1).expand(-1, seq_len, -1)
        
        attention_input = torch.cat([
            keys, query_expanded,
            keys * query_expanded,
            keys - query_expanded
        ], dim=-1)
        
        # æ—¶é—´è¡°å‡æƒé‡
        positions = torch.arange(seq_len, device=keys.device).float()
        time_weights = torch.exp(self.time_decay * (positions - seq_len + 1))
        
        head_outputs = []
        all_weights = []  # æ”¶é›†æ‰€æœ‰å¤´çš„æ³¨æ„åŠ›æƒé‡
        for head in self.attention_heads:
            scores = head(attention_input).squeeze(-1)
            scores = scores * time_weights.unsqueeze(0)
            
            if keys_mask is not None:
                scores = scores.masked_fill(~keys_mask.bool(), -1e9)
            
            weights = F.softmax(scores, dim=-1)
            all_weights.append(weights)
            output = torch.sum(weights.unsqueeze(-1) * keys, dim=1)
            head_outputs.append(output)
        
        combined = torch.stack(head_outputs, dim=1).mean(dim=1)
        output = self.output_proj(combined)
        
        # è¿”å›å¹³å‡æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºå¯è§†åŒ–å’Œhybrid_rankerï¼‰
        avg_weights = torch.stack(all_weights, dim=1).mean(dim=1)  # [B, seq_len]
        return output, avg_weights


# ========================================
# æ”¹è¿›ç‰ˆ DIN æ¨¡å‹
# ========================================

class DINRichImproved(nn.Module):
    """
    æ”¹è¿›ç‰ˆä¸°å¯Œç‰¹å¾ DIN
    
    æ”¯æŒä¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶å˜ä½“ã€‚
    """
    
    def __init__(
        self,
        num_items,
        num_users,
        feature_dims,
        embedding_dim=64,
        feature_embedding_dim=16,
        attention_type='base',  # 'base', 'time_decay', 'multi_head', 'full'
        mlp_hidden_dims=[256, 128, 64],
        dropout_rate=0.2,
        num_heads=4,
        time_decay=0.1
    ):
        super(DINRichImproved, self).__init__()
        
        self.embedding_dim = embedding_dim
        self.feature_embedding_dim = feature_embedding_dim
        self.attention_type = attention_type
        
        # åµŒå…¥å±‚
        self.item_embedding = nn.Embedding(num_items + 1, embedding_dim, padding_idx=0)
        self.user_embedding = nn.Embedding(num_users + 1, feature_embedding_dim)
        self.genre_embedding = nn.Embedding(feature_dims.get('primary_genre', 20) + 1, feature_embedding_dim, padding_idx=0)
        self.year_embedding = nn.Embedding(feature_dims.get('year_bucket', 8) + 1, feature_embedding_dim, padding_idx=0)
        self.age_embedding = nn.Embedding(feature_dims.get('age_bucket', 10) + 1, feature_embedding_dim)
        self.gender_embedding = nn.Embedding(3, feature_embedding_dim)
        self.occupation_embedding = nn.Embedding(feature_dims.get('occupation', 25) + 1, feature_embedding_dim)
        
        # åºåˆ—ç‰¹å¾ç»´åº¦
        self.seq_feature_dim = embedding_dim + 2 * feature_embedding_dim
        
        # é€‰æ‹©æ³¨æ„åŠ›æœºåˆ¶
        if attention_type == 'base':
            from models import AttentionLayer
            self.attention = AttentionLayer(self.seq_feature_dim, [64, 32])
        elif attention_type == 'time_decay':
            self.attention = TimeDecayRichAttention(self.seq_feature_dim, [64, 32], time_decay)
        elif attention_type == 'multi_head':
            self.attention = MultiHeadRichAttention(self.seq_feature_dim, num_heads, [64, 32])
        elif attention_type == 'full':
            self.attention = TimeDecayMultiHeadRichAttention(self.seq_feature_dim, num_heads, [64, 32], time_decay)
        else:
            raise ValueError(f"Unknown attention type: {attention_type}")
        
        # MLP
        mlp_input_dim = (
            self.seq_feature_dim +  # ç”¨æˆ·å…´è¶£
            self.seq_feature_dim +  # ç›®æ ‡ç‰©å“
            feature_embedding_dim +  # ç”¨æˆ·
            feature_embedding_dim * 3  # å¹´é¾„ + æ€§åˆ« + èŒä¸š
        )
        
        mlp_layers = []
        prev_dim = mlp_input_dim
        for hidden_dim in mlp_hidden_dims:
            mlp_layers.append(nn.Linear(prev_dim, hidden_dim))
            mlp_layers.append(nn.BatchNorm1d(hidden_dim))
            mlp_layers.append(nn.PReLU())
            mlp_layers.append(nn.Dropout(dropout_rate))
            prev_dim = hidden_dim
        mlp_layers.append(nn.Linear(prev_dim, 1))
        
        self.mlp = nn.Sequential(*mlp_layers)
        
        self._init_weights()
    
    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0, std=0.01)
    
    def forward(self, batch):
        # å†å²åºåˆ—åµŒå…¥
        seq_item_emb = self.item_embedding(batch['item_seq'])
        seq_genre_emb = self.genre_embedding(batch['history_genres'])
        seq_year_emb = self.year_embedding(batch['history_years'])
        seq_combined = torch.cat([seq_item_emb, seq_genre_emb, seq_year_emb], dim=-1)
        
        # ç›®æ ‡ç‰©å“åµŒå…¥
        target_item_emb = self.item_embedding(batch['target_item'])
        target_genre_emb = self.genre_embedding(batch['item_genre'])
        target_year_emb = self.year_embedding(batch['item_year'])
        target_combined = torch.cat([target_item_emb, target_genre_emb, target_year_emb], dim=-1)
        
        # æ³¨æ„åŠ›
        user_interest, _ = self.attention(target_combined, seq_combined, batch['item_seq_mask'])
        
        # ç”¨æˆ·ç‰¹å¾
        user_emb = self.user_embedding(batch['user_id'])
        age_emb = self.age_embedding(batch['user_age'])
        gender_emb = self.gender_embedding(batch['user_gender'])
        occupation_emb = self.occupation_embedding(batch['user_occupation'])
        
        # æ‹¼æ¥
        features = torch.cat([
            user_interest, target_combined,
            user_emb, age_emb, gender_emb, occupation_emb
        ], dim=-1)
        
        return self.mlp(features).squeeze(-1)


# ========================================
# ä¸»å®éªŒ
# ========================================

def run_experiment(dataset_name='ml-100k'):
    """
    è¿è¡Œæ¶ˆèå®éªŒçš„ä¸»å‡½æ•°
    
    Args:
        dataset_name: æ•°æ®é›†åç§°ï¼Œ'ml-100k' æˆ– 'ml-1m'
    """
    print("=" * 80)
    print("å®éªŒä¸‰ï¼ˆå¢å¼ºç‰ˆï¼‰ï¼šDIN æ”¹è¿›æ¶ˆèå®éªŒ")
    print("=" * 80)
    
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"è®¾å¤‡: {DEVICE}")
    
    # å®éªŒå‚æ•°
    MAX_SEQ_LENGTH = 50
    EPOCHS = 20
    BATCH_SIZE = 256
    EMBEDDING_DIM = 64
    
    # æ¶ˆèé…ç½®
    ABLATION_CONFIGS = [
        {'name': 'DIN-Rich-Base', 'attention_type': 'base', 'description': 'ä¸°å¯Œç‰¹å¾ + åŸºç¡€æ³¨æ„åŠ›'},
        {'name': 'DIN-Rich-TimeDec', 'attention_type': 'time_decay', 'description': '+ æ—¶é—´è¡°å‡æ³¨æ„åŠ›'},
        {'name': 'DIN-Rich-MultiHead', 'attention_type': 'multi_head', 'description': '+ å¤šå¤´æ³¨æ„åŠ›'},
        {'name': 'DIN-Rich-Full', 'attention_type': 'full', 'description': 'æ—¶é—´è¡°å‡ + å¤šå¤´æ³¨æ„åŠ›'},
    ]
    
    RESULTS_DIR = os.path.join(os.path.dirname(__file__), 'results')
    os.makedirs(RESULTS_DIR, exist_ok=True)
    
    results = []
    start_time = datetime.now()
    
    print(f"\nå¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æ¶ˆèé…ç½®æ•°: {len(ABLATION_CONFIGS)}")
    print()
    
    # åŠ è½½æ•°æ®
    print("ğŸ“¦ åŠ è½½æ•°æ®...")
    train_loader, valid_loader, test_loader, dataset_info, fp = get_rich_dataloaders(
        data_dir='./data',
        dataset_name=dataset_name,
        max_seq_length=MAX_SEQ_LENGTH,
        batch_size=BATCH_SIZE
    )
    
    for config in ABLATION_CONFIGS:
        print("\n" + "=" * 80)
        print(f"ğŸš€ {config['name']}: {config['description']}")
        print("=" * 80)
        
        try:
            model = DINRichImproved(
                num_items=dataset_info['num_items'],
                num_users=dataset_info['num_users'],
                feature_dims=dataset_info['feature_dims'],
                embedding_dim=EMBEDDING_DIM,
                attention_type=config['attention_type'],
                mlp_hidden_dims=[256, 128, 64],
                dropout_rate=0.2
            )
            
            trainer = RichTrainer(model=model, device=DEVICE)
            
            t1 = time.time()
            train_result = trainer.fit(
                train_loader=train_loader,
                valid_loader=valid_loader,
                epochs=EPOCHS,
                early_stopping_patience=5,
                show_progress=True
            )
            train_time = time.time() - t1
            
            test_metrics = trainer.evaluate(test_loader)
            speed = measure_inference_speed_rich(model, test_loader, DEVICE)
            
            results.append({
                'variant': config['name'],
                'description': config['description'],
                'test_auc': test_metrics['auc'],
                'test_logloss': test_metrics['logloss'],
                'best_valid_auc': train_result['best_valid_auc'],
                'train_time_sec': train_time,
                'qps': speed['qps'],
                'status': 'success'
            })
            
            print(f"\nâœ… {config['name']} å®Œæˆ!")
            print(f"   Test AUC: {test_metrics['auc']:.4f}")
            print(f"   Test LogLoss: {test_metrics['logloss']:.4f}")
            print(f"   QPS: {speed['qps']:.0f}")
            
        except Exception as e:
            print(f"âŒ {config['name']} é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            
            results.append({
                'variant': config['name'],
                'description': config['description'],
                'test_auc': None,
                'test_logloss': None,
                'best_valid_auc': None,
                'train_time_sec': None,
                'qps': None,
                'status': f'error: {str(e)[:100]}'
            })
    
    # å®Œæˆ
    end_time = datetime.now()
    total_time = (end_time - start_time).total_seconds()
    
    # ä¿å­˜ç»“æœ
    df_results = pd.DataFrame(results)
    results_file = os.path.join(RESULTS_DIR, f'experiment3_{dataset_name}_results.csv')
    df_results.to_csv(results_file, index=False)
    
    print("\n" + "=" * 80)
    print("ğŸ‰ å®éªŒä¸‰å®Œæˆ!")
    print("=" * 80)
    
    # å¯è§†åŒ–
    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–...")
    df_success = df_results[df_results['status'] == 'success'].copy()
    
    if len(df_success) > 0:
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
        
        # AUC å¯¹æ¯”
        bars = axes[0].bar(
            range(len(df_success)), 
            df_success['test_auc'],
            color=colors[:len(df_success)]
        )
        axes[0].set_xticks(range(len(df_success)))
        axes[0].set_xticklabels(df_success['variant'], rotation=20, ha='right')
        axes[0].set_ylabel('Test AUC', fontsize=12)
        axes[0].set_title('æ¶ˆèå®éªŒ: AUC å¯¹æ¯”', fontsize=14, fontweight='bold')
        for bar, val in zip(bars, df_success['test_auc']):
            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,
                        f'{val:.4f}', ha='center', va='bottom', fontsize=9)
        
        # æ”¹è¿›å¹…åº¦
        base_auc = df_success[df_success['variant'] == 'DIN-Rich-Base']['test_auc'].values[0]
        improvements = [(auc - base_auc) / base_auc * 100 for auc in df_success['test_auc']]
        
        bars = axes[1].bar(
            range(len(df_success)), 
            improvements,
            color=colors[:len(df_success)]
        )
        axes[1].set_xticks(range(len(df_success)))
        axes[1].set_xticklabels(df_success['variant'], rotation=20, ha='right')
        axes[1].set_ylabel('ç›¸å¯¹åŸºçº¿æå‡ (%)', fontsize=12)
        axes[1].set_title('æ¶ˆèå®éªŒ: æ”¹è¿›å¹…åº¦', fontsize=14, fontweight='bold')
        axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)
        for bar, val in zip(bars, improvements):
            axes[1].text(bar.get_x() + bar.get_width()/2,
                        bar.get_height() + 0.1 if val >= 0 else bar.get_height() - 0.3,
                        f'{val:.2f}%', ha='center', va='bottom', fontsize=9)
        
        # QPS å¯¹æ¯”
        bars = axes[2].bar(
            range(len(df_success)), 
            df_success['qps'],
            color=colors[:len(df_success)]
        )
        axes[2].set_xticks(range(len(df_success)))
        axes[2].set_xticklabels(df_success['variant'], rotation=20, ha='right')
        axes[2].set_ylabel('QPS', fontsize=12)
        axes[2].set_title('æ¶ˆèå®éªŒ: æ¨ç†é€Ÿåº¦', fontsize=14, fontweight='bold')
        for bar, val in zip(bars, df_success['qps']):
            axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,
                        f'{val:.0f}', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        plot_file = os.path.join(RESULTS_DIR, f'experiment3_{dataset_name}_plot.png')
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        print(f"âœ… å›¾è¡¨å·²ä¿å­˜: {plot_file}")
        plt.close()
    else:
        base_auc = 0
        improvements = []
    
    # æŠ¥å‘Š
    report = {
        'experiment': 'Experiment 3 (Rich Features): DIN Improvement Ablation Study',
        'dataset': dataset_name,
        'ablation_configs': [c['name'] for c in ABLATION_CONFIGS],
        'features_used': [
            'item_id', 'user_id',
            'history_genres', 'history_years',
            'item_genre', 'item_year',
            'user_age', 'user_gender', 'user_occupation'
        ],
        'total_time_seconds': total_time,
        'results': results
    }
    
    if len(df_success) > 0:
        best_idx = df_success['test_auc'].idxmax()
        report['best_variant'] = df_success.loc[best_idx, 'variant']
        report['best_auc'] = float(df_success.loc[best_idx, 'test_auc'])
        report['baseline_auc'] = float(base_auc)
        report['max_improvement'] = float(max(improvements)) if improvements else 0
    
    report_file = os.path.join(RESULTS_DIR, f'experiment3_{dataset_name}_report.json')
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # æ‰“å°ç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“‹ å®éªŒç»“æœæ‘˜è¦")
    print("=" * 80)
    print(df_results[['variant', 'test_auc', 'test_logloss', 'qps']].to_string(index=False))
    
    if len(df_success) > 0:
        print("\nğŸ” å…³é”®å‘ç°:")
        print(f"   åŸºçº¿ AUC: {base_auc:.4f}")
        print(f"   æœ€ä½³å˜ä½“: {report.get('best_variant', 'N/A')} (AUC={report.get('best_auc', 0):.4f})")
        print(f"   æœ€å¤§æå‡: {report.get('max_improvement', 0):.2f}%")
        
        for _, row in df_success.iterrows():
            improvement = (row['test_auc'] - base_auc) / base_auc * 100
            print(f"   {row['variant']}: AUC={row['test_auc']:.4f} ({improvement:+.2f}%)")
    
    print("=" * 80)
    
    return df_results, report


# ========================================
# å…¥å£ç‚¹
# ========================================

if __name__ == '__main__':
    run_experiment('ml-100k')
